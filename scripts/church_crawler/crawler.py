#!/usr/bin/env python3
"""
êµì  í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
- god4u.dimode.co.kr ì‚¬ì´íŠ¸ì—ì„œ êµì  ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
- ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸ í›„ ì¿ í‚¤ë¥¼ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
1. ë¸Œë¼ìš°ì €ì—ì„œ god4u.dimode.co.kr ë¡œê·¸ì¸
2. F12 â†’ Application â†’ Cookiesì—ì„œ ì¿ í‚¤ ê°’ ë³µì‚¬
3. ì•„ë˜ COOKIES ë”•ì…”ë„ˆë¦¬ì— ë¶™ì—¬ë„£ê¸°
4. python crawler.py ì‹¤í–‰
"""

import requests
import json
import csv
import time
from datetime import datetime
from pathlib import Path

# ============================================================
# ì„¤ì •: ë¸Œë¼ìš°ì €ì—ì„œ ë³µì‚¬í•œ ì¿ í‚¤ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”
# ============================================================
COOKIES = {
    "ASP.NET_SessionId": "xgsyv0zp5q0b14znzorohsk2",
    "pastorinfo": "sUser=%c1%a4%bb%e7%b9%ab%bf%a4&sID=5700&sUserName=%c1%a4%bb%e7%b9%ab%bf%a4&sIsAdmin=1&churchid=god4u&chname=%c0%c7%c1%a4%ba%ce%c1%df%be%d3%b1%b3%c8%b8&sChurchAuth=OK&sCashAuth=OK&sSCashAuth=&sRangeAuth=OK&sSchoolAuth=OK&sYouthAuth=OK",
    "AUTOLOGIN": "YES",
}

# API ì„¤ì •
BASE_URL = "http://god4u.dimode.co.kr"
API_URL = f"{BASE_URL}/Handler/GetPersonListMobileJSon.asmx/GetPersonSearchListDefault"

# í˜ì´ì§€ ì„¤ì •
PAGE_SIZE = 100  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê°œìˆ˜ (ìµœëŒ€ 100 ê¶Œì¥)
DELAY_BETWEEN_PAGES = 0.5  # í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)

# ì¶œë ¥ í•„ë“œ ì„¤ì • (ì›í•˜ëŠ” í•„ë“œë§Œ ì„ íƒ)
OUTPUT_FIELDS = [
    ("id", "êµì ë²ˆí˜¸"),
    ("name", "ì´ë¦„"),
    ("sex", "ì„±ë³„"),
    ("birth", "ìƒë…„ì›”ì¼"),
    ("age", "ë‚˜ì´"),
    ("cvname", "ì§ë¶„1"),
    ("cvname1", "ì§ë¶„2"),
    ("handphone", "í•¸ë“œí°"),
    ("tel", "ì „í™”"),
    ("addr", "ì£¼ì†Œ"),
    ("zipcode", "ìš°í¸ë²ˆí˜¸"),
    ("sido", "ì‹œë„"),
    ("gugun", "êµ¬êµ°"),
    ("dong", "ë™"),
    ("email", "ì´ë©”ì¼"),
    ("state", "ìƒíƒœ"),
    ("state1", "ê·¸ë£¹"),
    ("state3", "ì¶œì„ìƒíƒœ"),
    ("regday", "ë“±ë¡ì¼"),
    ("ran1", "ê°€ì¡±"),
    ("carnum", "ì°¨ëŸ‰ë²ˆí˜¸"),
    ("etc", "ê¸°íƒ€"),
]


def create_payload(page: int = 1, page_size: int = PAGE_SIZE, search_name: str = "") -> dict:
    """API ìš”ì²­ í˜ì´ë¡œë“œ ìƒì„±"""
    return {
        "paramName": search_name,
        "paramEName": "",
        "paramIds": "",
        "paramFree1": "",
        "paramFree2": "",
        "paramFree3": "",
        "paramFree4": "",
        "paramFree5": "",
        "paramFree6": "",
        "paramFree7": "",
        "paramFree8": "",
        "paramFree9": "",
        "paramFree10": "",
        "paramFree11": "",
        "paramFree12": "",
        "paramRange": "",
        "paramRange1": "",
        "paramRange2": "",
        "paramRange3": "",
        "paramRvname": "",
        "paramSection1": "",
        "paramSection2": "",
        "paramSection3": "",
        "paramSection4": "",
        "paramRvname2": "",
        "paramCoreChk": "",
        "paramCarNum": "",
        "paramGJeon": "",
        "paramLastSchool": "",
        "paramOffName": "",
        "paramGJeon1": "",
        "paramCvname": "",
        "paramCvname1": "",
        "paramState": "",
        "paramState1": "",
        "paramState3": "",
        "encryptOpt": "ALL",
        "rangeLimitUse": "false",  # falseë¡œ ì„¤ì •í•˜ì—¬ ì „ì²´ ì¡°íšŒ
        "paramPage": str(page),
        "paramPageSize": str(page_size),
        "paramOrder": "NAME",
        "paramOrder2": "",
        "paramOrderAsc": "ASC",
        "paramOrder2Asc": "ASC",
        "paramPType": "P",
        "paramAddr": "",
        "paramRegDateS": "",
        "paramRegDateE": "",
    }


def fetch_page(session: requests.Session, page: int, page_size: int = PAGE_SIZE) -> dict:
    """í•œ í˜ì´ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    headers = {
        "Content-Type": "application/json; charset=UTF-8",
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "X-Requested-With": "XMLHttpRequest",
        "Origin": BASE_URL,
        "Referer": f"{BASE_URL}/WebMobile/WebChurch/RangeList.cshtml",
    }

    payload = create_payload(page=page, page_size=page_size)

    response = session.post(API_URL, json=payload, headers=headers)
    response.raise_for_status()

    data = response.json()

    # "d" í•„ë“œ ì•ˆì— JSON ë¬¸ìì—´ì´ ë“¤ì–´ìˆìŒ
    if "d" in data:
        inner_data = json.loads(data["d"])
        return inner_data

    return data


def crawl_all(cookies: dict = None) -> list:
    """ì „ì²´ êµì  í¬ë¡¤ë§"""
    if cookies is None:
        cookies = COOKIES

    # ì¿ í‚¤ ê²€ì¦
    if "ì—¬ê¸°ì—" in str(cookies.values()):
        print("âŒ ì˜¤ë¥˜: ì¿ í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        print("   ë¸Œë¼ìš°ì € F12 â†’ Application â†’ Cookiesì—ì„œ ì¿ í‚¤ ê°’ì„ ë³µì‚¬í•˜ì„¸ìš”.")
        return []

    session = requests.Session()
    session.cookies.update(cookies)

    all_persons = []
    page = 1

    # ì²« í˜ì´ì§€ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
    print("ğŸ” ì „ì²´ êµì¸ ìˆ˜ í™•ì¸ ì¤‘...")
    first_page = fetch_page(session, page=1, page_size=PAGE_SIZE)

    total_count = int(first_page.get("totalcount", 0))
    total_pages = int(first_page.get("totalpage", 1))

    print(f"ğŸ“Š ì „ì²´ êµì¸: {total_count}ëª…, ì´ í˜ì´ì§€: {total_pages}")

    # ì²« í˜ì´ì§€ ë°ì´í„° ì¶”ê°€
    persons = first_page.get("personInfo", [])
    all_persons.extend(persons)
    print(f"   í˜ì´ì§€ 1/{total_pages} ì™„ë£Œ ({len(persons)}ëª…)")

    # ë‚˜ë¨¸ì§€ í˜ì´ì§€ í¬ë¡¤ë§
    for page in range(2, total_pages + 1):
        time.sleep(DELAY_BETWEEN_PAGES)

        try:
            page_data = fetch_page(session, page=page, page_size=PAGE_SIZE)
            persons = page_data.get("personInfo", [])
            all_persons.extend(persons)
            print(f"   í˜ì´ì§€ {page}/{total_pages} ì™„ë£Œ ({len(persons)}ëª…)")
        except Exception as e:
            print(f"   âš ï¸ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
            continue

    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_persons)}ëª…")
    return all_persons


def save_to_csv(persons: list, filename: str = None) -> str:
    """CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not persons:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"êµì _{timestamp}.csv"

    output_path = Path(__file__).parent / filename

    with open(output_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)

        # í—¤ë” ì‘ì„±
        headers = [display_name for _, display_name in OUTPUT_FIELDS]
        writer.writerow(headers)

        # ë°ì´í„° ì‘ì„±
        for person in persons:
            row = [person.get(field_name, "") or "" for field_name, _ in OUTPUT_FIELDS]
            writer.writerow(row)

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    return str(output_path)


def save_to_json(persons: list, filename: str = None) -> str:
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    if not persons:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"êµì _{timestamp}.json"

    output_path = Path(__file__).parent / filename

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(persons, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    return str(output_path)


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 50)
    print("ğŸ›ï¸  êµì  í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)

    # í¬ë¡¤ë§ ì‹¤í–‰
    persons = crawl_all()

    if persons:
        # CSV ì €ì¥
        save_to_csv(persons)

        # JSONë„ ì €ì¥ (ì„ íƒ)
        save_to_json(persons)

    print("\n" + "=" * 50)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 50)


if __name__ == "__main__":
    main()
